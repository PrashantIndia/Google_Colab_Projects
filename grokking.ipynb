{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Grokking in Deep Learning\n",
        "\n",
        "Grokking is a term used to describe a specific learning behavior in deep learning models.  Instead of a gradual increase in performance, the model demonstrates a sudden and significant improvement in its ability to generalize to unseen data after a period of seemingly slow progress.\n",
        "\n",
        "**How Grokking Works**\n",
        "\n",
        "While the exact mechanisms are still being researched, grokking is often observed in models with a large number of parameters (even more than the data points available). It's believed that the model initially focuses on memorizing the training data. Then, through continued training and optimization, it transitions from memorization to understanding the underlying patterns and relationships within the data.\n",
        "\n",
        "**Identifying Grokking in Your Model**\n",
        "\n",
        "The provided code helps you visualize the training process and identify potential grokking. After running the code:\n",
        "\n",
        "1. **Examine the Charts:** The code generates plots showing the model's performance on both the training data and a separate set of validation data.\n",
        "2. **Look for a Sharp Increase:** Grokking is characterized by a sudden and significant jump in the validation accuracy line, often after a period where it remains relatively flat.\n",
        "\n",
        "**What if My Model Doesn't Grok?**\n",
        "\n",
        "If your model doesn't exhibit grokking, don't worry! It's not a guaranteed phenomenon. The research paper on grokking suggests experimenting with these parameters to potentially encourage it:\n",
        "\n",
        "* **Dataset Size:** Try training with varying sizes of your training data. Smaller datasets might lead to faster grokking, but potentially at the cost of overall performance.\n",
        "* **Weight Decay:** This regularization technique, which penalizes large weights in the model, has been shown to promote grokking. Experiment with different weight decay values in your optimizer.\n",
        "* **Noise Injection:** Adding a small amount of noise during training, either to the input data or the model's gradients, can sometimes help the model escape from poor solutions and find those that generalize better.\n",
        "\n",
        "\n",
        "**Papers and Github**\n",
        "\n",
        "In this you will find grokking paper and the code of OpenAI and other person that try to replicate the code (the code made by others are much simpler to understand)\n",
        "- https://paperswithcode.com/paper/grokking-generalization-beyond-overfitting-on\n",
        "\n",
        "Here is a blog (it's in french because i speak french lol but use google translate)\n",
        "- https://scienceetonnante.substack.com/p/grokking-les-modeles-dia-sont-ils\n",
        "\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Grokking is not a magic way to train a model so the code is not that special but you can change the datasize, the complexity of the data and the learning algo to try helping it to better understand.\n",
        "***The more you dataset is complex the more you need to let the model train for a long long long time like more then 10000 step maybe more.***"
      ],
      "metadata": {
        "id": "3nZcXfe7urb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz3tELACVws9",
        "outputId": "10dd0b3f-27f0-46ac-d78e-0d0fe19e01aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from math import ceil\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "\n",
        "def load_data(directory):\n",
        "    \"\"\"Loads JSON data from a specified directory.\n",
        "\n",
        "    Args:\n",
        "        directory (str): The path to the directory containing JSON files.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each representing data from a JSON file.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.json'):\n",
        "            with open(os.path.join(directory, filename), 'r') as f:\n",
        "                data.append(json.load(f))\n",
        "    return data\n",
        "\n",
        "def extract_pairs(data):\n",
        "    \"\"\"Extracts input-output pairs from the loaded data.\n",
        "\n",
        "    Args:\n",
        "        data (list): The list of dictionaries containing data from JSON files.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Two lists - inputs and outputs - extracted from the data.\n",
        "    \"\"\"\n",
        "    inputs, outputs = [], []\n",
        "    for task in data:\n",
        "        for pair in task['train']:  # Assumes 'train' key holds the pairs\n",
        "            inputs.append(pair['input'])\n",
        "            outputs.append(pair['output'])\n",
        "    return inputs, outputs\n",
        "\n",
        "def linearize_grid(grid):\n",
        "    \"\"\"Linearizes a 2D grid into a 1D sequence.\n",
        "\n",
        "    Args:\n",
        "        grid (list of lists): The 2D grid to linearize.\n",
        "\n",
        "    Returns:\n",
        "        list: The linearized 1D sequence.\n",
        "    \"\"\"\n",
        "    return [cell for row in grid for cell in row]\n",
        "\n",
        "def pad_sequence(sequence, max_length, pad_value=0):\n",
        "    \"\"\"Pads a sequence to a specified maximum length.\n",
        "\n",
        "    Args:\n",
        "        sequence (list): The sequence to pad.\n",
        "        max_length (int): The desired maximum length.\n",
        "        pad_value (int, optional): The value used for padding. Defaults to 0.\n",
        "\n",
        "    Returns:\n",
        "        list: The padded sequence.\n",
        "    \"\"\"\n",
        "    return sequence + [pad_value] * (max_length - len(sequence))\n",
        "\n",
        "def tokenize_sequence(sequence):\n",
        "    \"\"\"Tokenizes a sequence by converting elements to integers.\n",
        "\n",
        "    Args:\n",
        "        sequence (list): The sequence to tokenize.\n",
        "\n",
        "    Returns:\n",
        "        list: The tokenized sequence.\n",
        "    \"\"\"\n",
        "    return [int(element) for element in sequence]\n",
        "\n",
        "def create_dataloader(inputs, outputs, batch_size):\n",
        "    \"\"\"Creates a PyTorch DataLoader from input-output tensors.\n",
        "\n",
        "    Args:\n",
        "        inputs (list): List of input sequences.\n",
        "        outputs (list): List of output sequences.\n",
        "        batch_size (int): The batch size for the DataLoader.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.data.DataLoader: The created DataLoader.\n",
        "    \"\"\"\n",
        "    input_tensors = torch.tensor(inputs, dtype=torch.long)\n",
        "    output_tensors = torch.tensor(outputs, dtype=torch.long)\n",
        "    dataset = TensorDataset(input_tensors, output_tensors)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# --- Model Definition (Transformer) ---\n",
        "\n",
        "class DecoderBlock(torch.nn.Module):\n",
        "    def __init__(self, dim_model: int, n_heads: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn = nn.MultiheadAttention(dim_model, n_heads)\n",
        "        self.self_attn_norm = nn.LayerNorm(dim_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(dim_model, dim_model * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim_model * 4, dim_model)\n",
        "        )\n",
        "        self.ffn_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        attn_mask = torch.full(\n",
        "            (len(x), len(x)), -float(\"Inf\"), device=x.device, dtype=x.dtype\n",
        "        )\n",
        "        attn_mask = torch.triu(attn_mask, diagonal=1)\n",
        "\n",
        "        a1, _ = self.self_attn(x, x, x, attn_mask=attn_mask)\n",
        "        a1 = self.self_attn_norm(x + a1)\n",
        "        a2 = self.ffn(a1)\n",
        "        a2 = self.ffn_norm(a1 + a2)\n",
        "\n",
        "        return a2\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "    def __init__(self, num_layers: int, dim_model: int, num_heads: int, num_tokens: int, seq_len: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embeddings = nn.Embedding(num_tokens, dim_model)\n",
        "        self.position_embeddings = nn.Embedding(seq_len, dim_model)\n",
        "        self.model = nn.Sequential(\n",
        "            *[DecoderBlock(dim_model, num_heads) for _ in range(num_layers)],\n",
        "            nn.LayerNorm(dim_model),\n",
        "            nn.Linear(dim_model, num_tokens)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs: Tensor):\n",
        "        batch_size, context_len = inputs.shape\n",
        "\n",
        "        token_embedding = self.token_embeddings(inputs)\n",
        "\n",
        "        positions = repeat(torch.arange(context_len, device=inputs.device), \"p -> b p\", b = batch_size)\n",
        "        position_embedding = self.position_embeddings(positions)\n",
        "\n",
        "        embedding = token_embedding + position_embedding\n",
        "\n",
        "        embedding = rearrange(embedding, 'b s d -> s b d')\n",
        "\n",
        "        return self.model(embedding)\n",
        "\n",
        "# --- Training and Evaluation Functions ---\n",
        "\n",
        "def train(model, train_loader, optimizer, scheduler, device, num_steps, accumulation_steps, loss_history, accuracy_history):\n",
        "    \"\"\"Trains the model for a specified number of steps.\n",
        "\n",
        "    Args:\n",
        "        model: The Transformer model to train.\n",
        "        train_loader: DataLoader for the training data.\n",
        "        optimizer: The optimizer used for training.\n",
        "        scheduler: The learning rate scheduler.\n",
        "        device: The device to train on (CPU or GPU).\n",
        "        num_steps (int): Total training steps.\n",
        "        accumulation_steps (int): Number of steps to accumulate gradients before updating.\n",
        "        loss_history (list): List to store training loss history.\n",
        "        accuracy_history (list): List to store training accuracy history.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_loader, total=len(train_loader), leave=False)):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, labels = batch\n",
        "        labels = labels.view(-1)  # Reshape labels to 1D\n",
        "\n",
        "        output = model(inputs)[-1,:,:]  # Get output from the last decoder layer\n",
        "        loss = criterion(output, labels) / accumulation_steps\n",
        "        acc = (torch.argmax(output, dim=1) == labels).sum().item() / len(labels)\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # --- More Frequent Logging ---\n",
        "        if (step + 1) % 10 == 0:  # Log every 10 steps (adjust as needed)\n",
        "            loss_history.append(loss.item() * accumulation_steps)\n",
        "            accuracy_history.append(acc)\n",
        "\n",
        "        # --- Early Stopping (Optional) ---\n",
        "        # if len(loss_history) > 200 and loss_history[-1] < 0.05 and np.mean(loss_history[-100:]) - np.mean(loss_history[-200:-100]) < 1e-4:\n",
        "        #     print(\"Early stopping triggered.\")\n",
        "        #     break\n",
        "\n",
        "        if step >= num_steps:\n",
        "            break\n",
        "\n",
        "def evaluate(model, val_loader, device):\n",
        "    \"\"\"Evaluates the model on the validation set.\n",
        "\n",
        "    Args:\n",
        "        model: The Transformer model to evaluate.\n",
        "        val_loader: DataLoader for the validation data.\n",
        "        device: The device to evaluate on (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average validation loss and validation accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    correct = 0\n",
        "    total_loss = 0.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs, labels = batch\n",
        "\n",
        "            # Reshape labels to 1D tensor\n",
        "            labels = labels.view(-1)\n",
        "\n",
        "            output = model(inputs)[-1,:,:]\n",
        "            loss = criterion(output, labels)\n",
        "            total_loss += loss.item() * len(labels)\n",
        "            correct += (torch.argmax(output, dim=1) == labels).sum().item()\n",
        "\n",
        "    acc = correct / len(val_loader.dataset)\n",
        "    avg_loss = total_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}, Validation Accuracy: {acc:.4f}\")\n",
        "    return avg_loss, acc\n",
        "\n",
        "# --- Plotting with Enhanced Analysis ---\n",
        "\n",
        "def plot_metrics(loss_history, accuracy_history, val_loss_history, val_accuracy_history):\n",
        "    \"\"\"Plots training and validation metrics with potential grokking highlights.\n",
        "\n",
        "    Args:\n",
        "        loss_history (list): Training loss history.\n",
        "        accuracy_history (list): Training accuracy history.\n",
        "        val_loss_history (list): Validation loss history.\n",
        "        val_accuracy_history (list): Validation accuracy history.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # --- Loss Plot ---\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(loss_history, label='Training Loss', alpha=0.7)\n",
        "    plt.plot(val_loss_history, label='Validation Loss', alpha=0.7)\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    # --- Highlight Potential Grokking Region (Loss) ---\n",
        "    if len(val_loss_history) > 100: # Adjust threshold as needed\n",
        "        min_val_loss_idx = np.argmin(val_loss_history)\n",
        "        if min_val_loss_idx > 50: # Check if the minimum is not too early\n",
        "            plt.axvspan(min_val_loss_idx - 50, min_val_loss_idx + 50, color='lightblue', alpha=0.5)\n",
        "\n",
        "    # --- Accuracy Plot ---\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(accuracy_history, label='Training Accuracy', alpha=0.7)\n",
        "    plt.plot(val_accuracy_history, label='Validation Accuracy', alpha=0.7)\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    # --- Highlight Potential Grokking Region (Accuracy) ---\n",
        "    if len(val_accuracy_history) > 100: # Adjust threshold as needed\n",
        "        max_val_acc_idx = np.argmax(val_accuracy_history)\n",
        "        if max_val_acc_idx > 50: # Check if the maximum is not too early\n",
        "            plt.axvspan(max_val_acc_idx - 50, max_val_acc_idx + 50, color='lightgreen', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "N_Z9ycCSW4Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Function ---\n",
        "\n",
        "def main():\n",
        "    # --- Configurations ---\n",
        "    num_layers = 3       # Number of decoder layers in the Transformer\n",
        "    dim_model = 64      # Model dimensionality\n",
        "    num_heads = 4       # Number of attention heads\n",
        "    num_tokens = 11      # Size of the vocabulary (number of distinct tokens)\n",
        "    batch_size = 8       # Batch size for training\n",
        "    learning_rate = 1e-3 # Learning rate\n",
        "    weight_decay = 1e-2  # Weight decay for regularization\n",
        "    num_steps = 1000     # Total training steps\n",
        "    accumulation_steps = 4 # Gradient accumulation steps\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # --- Data Loading ---\n",
        "    train_data = load_data('train')  # Load training data\n",
        "    eval_data = load_data('evaluation')  # Load evaluation data\n",
        "    train_inputs, train_outputs = extract_pairs(train_data)\n",
        "    eval_inputs, eval_outputs = extract_pairs(eval_data)\n",
        "    linearized_train_inputs = [linearize_grid(input_grid) for input_grid in train_inputs]\n",
        "    linearized_train_outputs = [linearize_grid(output_grid) for output_grid in train_outputs]\n",
        "    linearized_eval_inputs = [linearize_grid(input_grid) for input_grid in eval_inputs]\n",
        "    linearized_eval_outputs = [linearize_grid(output_grid) for output_grid in eval_outputs]\n",
        "    max_length = max(\n",
        "        max(len(seq) for seq in linearized_train_inputs + linearized_train_outputs),\n",
        "        max(len(seq) for seq in linearized_eval_inputs + linearized_eval_outputs)\n",
        "    )\n",
        "    padded_train_inputs = [pad_sequence(seq, max_length) for seq in linearized_train_inputs]\n",
        "    padded_train_outputs = [pad_sequence(seq, max_length) for seq in linearized_train_outputs]\n",
        "    padded_eval_inputs = [pad_sequence(seq, max_length) for seq in linearized_eval_inputs]\n",
        "    padded_eval_outputs = [pad_sequence(seq, max_length) for seq in linearized_eval_outputs]\n",
        "    tokenized_train_inputs = [tokenize_sequence(input_seq) for input_seq in padded_train_inputs]\n",
        "    tokenized_train_outputs = [tokenize_sequence(output_seq) for output_seq in padded_train_outputs]\n",
        "    tokenized_eval_inputs = [tokenize_sequence(input_seq) for input_seq in padded_eval_inputs]\n",
        "    tokenized_eval_outputs = [tokenize_sequence(output_seq) for output_seq in padded_eval_outputs]\n",
        "    train_loader = create_dataloader(tokenized_train_inputs, tokenized_train_outputs, batch_size)\n",
        "    eval_loader = create_dataloader(tokenized_eval_inputs, tokenized_eval_outputs, batch_size)\n",
        "\n",
        "    # --- Model, Optimizer, Scheduler Initialization ---\n",
        "    model = Transformer(\n",
        "        num_layers=num_layers,\n",
        "        dim_model=dim_model,\n",
        "        num_heads=num_heads,\n",
        "        num_tokens=num_tokens,\n",
        "        seq_len=max_length\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        betas=(0.9, 0.98),\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "        optimizer, start_factor=0.1, total_iters=9\n",
        "    )\n",
        "\n",
        "    num_epochs = ceil(num_steps / len(train_loader))\n",
        "    loss_history = []\n",
        "    accuracy_history = []\n",
        "    val_loss_history = []\n",
        "    val_accuracy_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        train(model, train_loader, optimizer, scheduler, device, num_steps, accumulation_steps, loss_history, accuracy_history)\n",
        "        val_loss, val_acc = evaluate(model, eval_loader, device)\n",
        "        val_loss_history.append(val_loss)\n",
        "        val_accuracy_history.append(val_acc)\n",
        "\n",
        "    plot_metrics(loss_history, accuracy_history, val_loss_history, val_accuracy_history)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "RUTUMcqQtO1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}